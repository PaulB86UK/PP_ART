---
title: 26/05/2020
permalink: /2020/27_5
---
[Go Back](https://paulb86uk.github.io/PP_ART.github.io/)

# 26th and 27th of May of 2020
## ['Age'] Column Transformations
### Strategy 1: Building classes with aproximately same amount of data on each class
I build 3 different blocks of coded to perform this task. The first one builds the classes limits in order to get a similar amoung of data on each class. 

```python
def bins_func(df,col):
  for i in range (20,30,1):
    num = len(df[col])/i
    if round(num) - num > 0.1:
      pass
    else:
      value = i
      break
  bins = []
  df = round(df.sort_values(by=col, ascending= True))
  for j  in range (0,len(df),value):
    bins.append(df[col].iloc[j])
  bins.append(df[col].iloc[len(df)-1])
  return bins
```
In the first part of this code I tried to forced each class to have n => 20. That is the reason why the range goes between 20 and 30, and the code will stop when the residual of the division is less than 0.1. In the second part,  I order the age values {A-Z}, and trigger for the value(the resulting i that has the lowest residual first), a kind of lookup method(iloc[j]), on the number that results after every moving a "value" range in the data. This way I found the class delimitators (similar amount of data in each) in the bins list.
The result is the following:

```python
bins = bins_func(age_train,'age')
bins
---------------------------------
[ 1., 17., 22., 25., 28., 31., 36., 38., 42., 47., 49., 54., 60., 80.]
```
The second part is to used this limits to actually build the classes columns. So in order to do that, I build this code:
```python
def continuos_num_to_cat_num(df,col,bins): #age_train,'Age'
    age_class = []
    for i in df[col]:
      i = int(i) #note: age should be a integer (?) xD
      if i <= bins[1]:
        age_class.append(0) #17
      elif i < bins[2]:
        age_class.append(1) #22
      elif i < bins[3]:
          age_class.append(2) #25
      elif i < bins[4]:
          age_class.append(3) #28
      elif i < bins[5]:
        age_class.append(4) #31
      elif i < bins[6]:
        age_class.append(5) #36
      elif i < bins[7]:
        age_class.append(6) #38
      elif i < bins[8]:
        age_class.append(7) #42
      elif i < bins[9]:
        age_class.append(8) #47
      elif i < bins[10]:
        age_class.append(9) #49
      elif i < bins[11]:
        age_class.append(10) #54
      elif i < bins[12]:
        age_class.append(11) #60
      elif i < bins[13]:
        age_class.append(12) #80
      else:
        age_class.append(12) #80
    return age_class
```
And to perform this two task, this is the 3rd block of code.
```python
def age_into_class(df,col):
  bins = bins_func(df,col)
  df['age_class'] = continuos_num_to_cat_num(df,col,bins)
```
And the result is the following:

```python
age_train['age_class'].value_counts()
-----------------------------
5     27
10    23
2     23
0     23
12    22
11    22
8     22
6     21
7     20
4     18
3     18
1     17
9     16
Name: age_class, dtype: int64
```
Checking this configuration for the XGBoost model as a classification problem, and after some little fine-tuning, I obtained the following results:
```python
              precision    recall  f1-score   support

           0       0.50      0.75      0.60         4
           1       0.00      0.00      0.00         4
           2       0.20      0.20      0.20         5
           3       0.00      0.00      0.00         4
           4       0.00      0.00      0.00         4
           5       0.25      0.33      0.29         6
           6       0.40      0.50      0.44         4
           7       0.20      0.25      0.22         4
           8       0.50      0.25      0.33         4
           9       0.00      0.00      0.00         3
          10       0.33      0.20      0.25         5
          11       0.11      0.25      0.15         4
          12       0.33      0.25      0.29         4

    accuracy                           0.24        55
   macro avg       0.22      0.23      0.21        55
weighted avg       0.22      0.24      0.22        55
```
This result were as not as good as I expected, so I started working a different strategy.

## Second Strategy
I explored the log10 distribution of the age data. I selected the data between the 0.99 and 2, which looked more normal-like distributed data, and used this data as the target y.
The code I used is the following:
```python
age_select = age_train[(np.log10(age_train['age']) > 0.99)]
age_select2 = age_select[(np.log10(age_select['age']) < 2)]
print('Result: \n' + str(age_select2['age'].count()/(age_train['age'].count()) * 100) + '% of data selected\n')
#Result: 
92.16061185468452% of data selected
```
So this strategy sacrifies an 8% of the data. As this time is a Regression problem I used the R2 score, to check how it performs. The result I obtained *after several hours of tuning* is the following:

```python
xgb_clf = XGBRegressor(booster = 'gbtree', verbose = 0, max_depth= 7, eta = 0.15, n_estimators = 2500, eval_metric ='rmse', 
                       min_child_weight=1, gamma = 0.40, max_delta_step= 1, subsample=0.75, colsample_bytree=0.5, objective = 'reg:squarederror', alpha = 0.8, scale_pos_weight=1, random_state= 79)
parameters= {'gamma': [0.15,0.20,0.25]}
XG_CLF= GridSearchCV(xgb_clf, parameters, n_jobs=-1, refit=True, cv=9, verbose=False, scoring= r2_scorer)
XG_CLF.fit(X_t,y2)
print('\nThe CV is: ' + str(9))
XG_Res = pd.concat([pd.DataFrame(XG_CLF.cv_results_["params"]),pd.DataFrame(XG_CLF.cv_results_["mean_test_score"], columns=["Score"])],axis=1)
XG_Res = XG_Res.sort_values(by='Score',ascending=False)
print(XG_Res.head(3))
----------------------------------------------------------------------------------
RESULTS:
The CV is: 9
   gamma     Score
1   0.20  0.205567
0   0.15  0.187664
2   0.25  0.171570
```
Again I expected to have a better score, but considering that it's a small amount of data, and altough there is  some correlation of some variables with the age, they might also correlate with each other, making difficult to deploy a good model.
Tomorrow I will continue with this work triying some neural networks, to see if they can improve a little this score.

UPDATE 31/5. After different tries with NN could not get a better score, it was a dead end. But I decided to get into a different competition, using all the codes I have up to know.
&nbsp;
Thanks for reading!!

### PP_ART

[Go Back](https://paulb86uk.github.io/PP_ART.github.io/)
